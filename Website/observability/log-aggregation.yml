# Log Aggregation Configuration for Project Beacon

# Fluent Bit Configuration for Log Collection
fluent_bit:
  service:
    flush: 1
    daemon: off
    log_level: info
    parsers_file: parsers.conf
    plugins_file: plugins.conf
    http_server: on
    http_listen: 0.0.0.0
    http_port: 2020

  inputs:
    - name: tail
      path: /var/log/beacon-runner/*.log
      parser: json
      tag: beacon.runner
      refresh_interval: 5
      
    - name: tail
      path: /var/log/beacon-runner/audit/*.log
      parser: json
      tag: beacon.audit
      refresh_interval: 5
      
    - name: systemd
      tag: beacon.system
      systemd_filter: _SYSTEMD_UNIT=beacon-runner.service

  filters:
    - name: parser
      match: beacon.*
      key_name: log
      parser: json
      reserve_data: true
      
    - name: modify
      match: beacon.*
      add:
        service: beacon-runner
        environment: ${ENVIRONMENT}
        region: ${FLY_REGION}
        
    - name: grep
      match: beacon.*
      exclude: level debug
      
    - name: throttle
      match: beacon.runner
      rate: 1000
      window: 300
      interval: 1s

  outputs:
    - name: loki
      match: beacon.*
      host: ${LOKI_HOST}
      port: 3100
      labels: job=beacon-runner,environment=${ENVIRONMENT}
      
    - name: es
      match: beacon.*
      host: ${ELASTICSEARCH_HOST}
      port: 9200
      index: beacon-runner-logs
      type: _doc
      
    - name: datadog
      match: beacon.*
      host: http-intake.logs.datadoghq.com
      tls: on
      compress: gzip
      apikey: ${DATADOG_API_KEY}

# Vector Configuration (Alternative to Fluent Bit)
vector:
  sources:
    beacon_logs:
      type: file
      include:
        - /var/log/beacon-runner/*.log
        - /var/log/beacon-runner/**/*.log
      read_from: beginning
      
    beacon_metrics:
      type: prometheus_scrape
      endpoints:
        - http://localhost:8080/metrics
      scrape_interval_secs: 30

  transforms:
    parse_json:
      type: remap
      inputs:
        - beacon_logs
      source: |
        . = parse_json!(.message)
        .timestamp = parse_timestamp!(.timestamp, "%Y-%m-%dT%H:%M:%S%.fZ")
        .service = "beacon-runner"
        .environment = "${ENVIRONMENT}"
        
    add_metadata:
      type: remap
      inputs:
        - parse_json
      source: |
        .host = get_hostname!()
        .region = "${FLY_REGION}"
        .version = "${APP_VERSION}"
        
    filter_sensitive:
      type: remap
      inputs:
        - add_metadata
      source: |
        if exists(.password) { del(.password) }
        if exists(.token) { del(.token) }
        if exists(.secret) { del(.secret) }
        if exists(.signature) { .signature = "***REDACTED***" }

  sinks:
    loki_sink:
      type: loki
      inputs:
        - filter_sensitive
      endpoint: ${LOKI_URL}
      labels:
        service: "{{ service }}"
        environment: "{{ environment }}"
        level: "{{ level }}"
        
    elasticsearch_sink:
      type: elasticsearch
      inputs:
        - filter_sensitive
      endpoint: ${ELASTICSEARCH_URL}
      index: beacon-runner-logs-%Y.%m.%d
      
    console_sink:
      type: console
      inputs:
        - filter_sensitive
      encoding:
        codec: json

# Grafana Loki Configuration
loki:
  server:
    http_listen_port: 3100
    
  ingester:
    lifecycler:
      address: 127.0.0.1
      ring:
        kvstore:
          store: inmemory
        replication_factor: 1
    chunk_idle_period: 5m
    chunk_retain_period: 30s
    
  schema_config:
    configs:
      - from: 2020-10-24
        store: boltdb
        object_store: filesystem
        schema: v11
        index:
          prefix: index_
          period: 168h
          
  storage_config:
    boltdb:
      directory: /loki/index
    filesystem:
      directory: /loki/chunks
      
  limits_config:
    enforce_metric_name: false
    reject_old_samples: true
    reject_old_samples_max_age: 168h

# Promtail Configuration for Loki
promtail:
  server:
    http_listen_port: 9080
    grpc_listen_port: 0
    
  positions:
    filename: /tmp/positions.yaml
    
  clients:
    - url: ${LOKI_URL}/loki/api/v1/push
      basic_auth:
        username: ${LOKI_USERNAME}
        password: ${LOKI_PASSWORD}
        
  scrape_configs:
    - job_name: beacon-runner
      static_configs:
        - targets:
            - localhost
          labels:
            job: beacon-runner
            service: beacon-runner
            environment: ${ENVIRONMENT}
            __path__: /var/log/beacon-runner/*.log
            
    - job_name: beacon-audit
      static_configs:
        - targets:
            - localhost
          labels:
            job: beacon-audit
            service: beacon-runner
            log_type: audit
            __path__: /var/log/beacon-runner/audit/*.log

# Elasticsearch Index Templates
elasticsearch:
  index_templates:
    beacon_logs:
      index_patterns:
        - "beacon-runner-logs-*"
      template:
        settings:
          number_of_shards: 1
          number_of_replicas: 1
          index.lifecycle.name: "beacon-logs-policy"
        mappings:
          properties:
            timestamp:
              type: date
            level:
              type: keyword
            message:
              type: text
              analyzer: standard
            service:
              type: keyword
            environment:
              type: keyword
            job_id:
              type: keyword
            trace_id:
              type: keyword
            request_id:
              type: keyword
            error:
              type: text
            duration_ms:
              type: long
            
  lifecycle_policies:
    beacon_logs_policy:
      phases:
        hot:
          actions:
            rollover:
              max_size: 1GB
              max_age: 1d
        warm:
          min_age: 7d
          actions:
            allocate:
              number_of_replicas: 0
        cold:
          min_age: 30d
          actions:
            allocate:
              number_of_replicas: 0
        delete:
          min_age: 90d
