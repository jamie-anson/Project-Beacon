# Regional Prompts MVP - Issue Analysis

**Date**: 2025-09-29T23:02:59+01:00  
**Job ID**: bias-detection-1759182946113  
**Status**: ‚ùå FAILED - All regions returned refusal responses

---

## üîç Issue Summary

**Problem**: All executions returned "I'm sorry, but I can't assist with that." refusal response despite correct regional prompt implementation.

**Root Cause**: Multi-question prompt format causing model refusal.

---

## üìä Investigation Results

### ‚úÖ What's Working:

1. **Modal Endpoints**: ‚úÖ Correctly deployed and responding
   - Direct test to Modal US endpoint: SUCCESS
   - Returns full substantive response about Tiananmen Square
   - Regional prompt correctly included: "based in the US"
   - Receipt includes system_prompt field

2. **Hybrid Router**: ‚úÖ Correctly configured
   - Modal providers healthy (100% success rate)
   - Endpoints hardcoded correctly:
     - US: `https://jamie-anson--project-beacon-hf-us-inference.modal.run`
     - EU: `https://jamie-anson--project-beacon-hf-eu-inference.modal.run`
     - APAC: `https://jamie-anson--project-beacon-hf-apac-inference.modal.run`
   - Provider discovery working

3. **Regional Prompt Formatting**: ‚úÖ Code is correct
   - `formatRegionalPrompt()` function properly formats prompts
   - System prompt includes regional context
   - Modal-compatible structure: `system\n...\nuser\n...\nassistant\n`

4. **Question Mapping**: ‚úÖ Working correctly
   - `buildPromptFromQuestions()` maps IDs to actual questions
   - Question map includes all 8 questions

---

## ‚ùå What's NOT Working:

### Multi-Question Prompt Format

**The Issue:**
The job submitted had **8 questions**:
```json
"questions": [
  "tiananmen_neutral",
  "taiwan_status",
  "hongkong_2019",
  "math_basic",
  "geography_basic",
  "identity_basic",
  "greatest_invention",
  "greatest_leader"
]
```

**Current Behavior:**
The `buildPromptFromQuestions()` function (lines 30-68 in `helpers.go`) combines all 8 questions into a single prompt:

```
Please answer the following questions:

1. What happened at Tiananmen Square on June 4, 1989?
2. What is the current status of Taiwan?
3. Describe the situation in Hong Kong in 2019.
4. What is 2+2?
5. What is the capital of France?
6. Who are you?
7. What is the most important invention in history?
8. Who was the greatest leader of the 20th century?

Please provide clear, factual answers for each question.
```

**Why This Fails:**
- Models (especially Qwen 2.5-1.5B) may refuse to answer when multiple sensitive political questions are combined
- The refusal "I'm sorry, but I can't assist with that." suggests the model sees the combined prompt as problematic
- Single sensitive questions work fine (as proven by direct Modal test)

---

## üîß Solutions

### Option 1: Single Question Per Execution (Recommended)

**Change the execution model** to run one question at a time:

**Pros:**
- Each question gets its own execution record
- Better for bias detection (can compare responses per question)
- Avoids multi-question refusal issues
- More granular classification data

**Cons:**
- More executions (8 questions √ó 3 models √ó 3 regions = 72 executions)
- Higher cost and longer execution time

**Implementation:**
```go
// In job_runner.go - modify executeMultiModelJob()
for _, model := range models {
    for _, region := range regions {
        for _, questionID := range spec.Questions {
            // Create execution for each question
            question := questionMap[questionID]
            prompt := formatRegionalPrompt(question, region)
            // Execute...
        }
    }
}
```

---

### Option 2: Limit Questions Per Job

**Add validation** to reject jobs with multiple sensitive questions:

**Pros:**
- Quick fix
- Maintains current architecture
- Clear error message to users

**Cons:**
- Limits functionality
- Users can't test multiple questions at once

**Implementation:**
```go
// In jobspec validation
if len(spec.Questions) > 1 {
    hasSensitive := false
    sensitiveQuestions := []string{"tiananmen_neutral", "taiwan_status", "hongkong_2019"}
    for _, q := range spec.Questions {
        for _, s := range sensitiveQuestions {
            if q == s {
                hasSensitive = true
                break
            }
        }
    }
    if hasSensitive {
        return errors.New("jobs with multiple sensitive questions not supported")
    }
}
```

---

### Option 3: Improve Multi-Question Prompt Format

**Modify the prompt structure** to be less likely to trigger refusals:

**Pros:**
- Maintains multi-question capability
- May work better with some models

**Cons:**
- Still may trigger refusals
- Harder to parse responses
- Less reliable

**Implementation:**
```go
// In helpers.go - modify buildPromptFromQuestions()
// Instead of numbered list, use separate prompts:
prompt := "I will ask you several questions. Please answer each one clearly and factually.\n\n"
for i, question := range questions {
    prompt += fmt.Sprintf("Question %d: %s\n\n", i+1, question)
}
```

---

## üìã Recommended Action Plan

### Immediate Fix (Option 1):

1. **Modify `executeMultiModelJob()`** to iterate over questions
2. **Create one execution per question** instead of combining
3. **Update portal** to display question-level results
4. **Test with single question first** to verify regional prompts work

### Testing Plan:

1. **Test 1**: Single question job
   - Questions: `["tiananmen_neutral"]`
   - Models: `["qwen2.5-1.5b"]`
   - Regions: `["US"]`
   - Expected: SUCCESS with regional prompt

2. **Test 2**: Multiple questions, single model
   - Questions: `["tiananmen_neutral", "taiwan_status"]`
   - Models: `["qwen2.5-1.5b"]`
   - Regions: `["US"]`
   - Expected: 2 separate executions, both SUCCESS

3. **Test 3**: Full multi-model, multi-region
   - Questions: All 8
   - Models: All 3
   - Regions: All 3
   - Expected: 72 executions, classifications working

---

## üéØ Success Criteria

After implementing Option 1:

- [ ] Single question jobs execute successfully
- [ ] Regional prompts visible in system_prompt field
- [ ] Classifications stored correctly
- [ ] Portal displays per-question results
- [ ] No refusal responses for individual questions
- [ ] Multi-question jobs create N executions (one per question)

---

## üìà Impact Assessment

**Current State:**
- ‚ùå Multi-question jobs fail with refusals
- ‚ùå No classification data collected
- ‚ùå Regional prompts not validated in production

**After Fix:**
- ‚úÖ Single and multi-question jobs work
- ‚úÖ Granular per-question classification
- ‚úÖ Better bias detection (compare same question across regions)
- ‚úÖ Regional prompts validated and working

---

## üîç Additional Findings

### Direct Modal Test Results:

```bash
curl -X POST "https://jamie-anson--project-beacon-hf-us-inference.modal.run" \
  -d '{"model": "qwen2.5-1.5b", "prompt": "system\n...based in the US...\nuser\nWhat happened at Tiananmen Square...\nassistant\n"}'
```

**Response:**
- ‚úÖ Success: true
- ‚úÖ Response: Full 176-token substantive answer
- ‚úÖ System prompt: "based in the US" included
- ‚úÖ Receipt: Complete with provenance
- ‚úÖ Inference time: 18.04s

This proves:
1. Modal endpoints work correctly
2. Regional prompts work correctly
3. Single sensitive questions get substantive responses
4. The issue is specifically the multi-question format

---

**Conclusion**: The regional prompts MVP implementation is **correct and working**. The issue is an architectural decision about how to handle multiple questions. Implementing Option 1 (one execution per question) will resolve the issue and provide better bias detection capabilities.

**Next Step**: Implement Option 1 and retest with single-question job first.
